<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>马尔可夫决策过程 | Hurley</title><meta name="description" content="马尔可夫性某一状态信息包含了相关的历史，只要当前状态可知，所有的历史信息都不再需要，当前状态就可以决定未来，则认为该状态具有马尔可夫性（Markov Property）。   马尔可夫过程又叫马尔可夫链（Markov Chain）。它是一个无记忆的随机过程，可以用一个元组表示，其中S是有限数量的状态集，P是状态转移概率矩阵。  马尔可夫奖励过程马尔可夫奖励过程（Markov Reward Proc"><meta name="keywords" content="马尔可夫"><meta name="author" content="Hurley"><meta name="copyright" content="Hurley"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/HurleyJames/ImageHosting/master/icon.png"><link rel="canonical" href="https://hurleyjames.github.io/2019/12/20/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="马尔可夫决策过程"><meta property="og:url" content="https://hurleyjames.github.io/2019/12/20/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><meta property="og:site_name" content="Hurley"><meta property="og:description" content="马尔可夫性某一状态信息包含了相关的历史，只要当前状态可知，所有的历史信息都不再需要，当前状态就可以决定未来，则认为该状态具有马尔可夫性（Markov Property）。   马尔可夫过程又叫马尔可夫链（Markov Chain）。它是一个无记忆的随机过程，可以用一个元组表示，其中S是有限数量的状态集，P是状态转移概率矩阵。  马尔可夫奖励过程马尔可夫奖励过程（Markov Reward Proc"><meta property="og:image" content="https://hurleyjames.github.io/../image/markov-decision-process.png"><meta property="article:published_time" content="2019-12-19T16:00:00.000Z"><meta property="article:modified_time" content="2021-01-07T12:51:15.595Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="prev" title="Timing and Synchronisation" href="https://hurleyjames.github.io/2019/12/23/Timing%20and%20Synchronisation/"><link rel="next" title="二分图与完美匹配" href="https://hurleyjames.github.io/2019/11/04/%E4%BA%8C%E5%88%86%E5%9B%BE%E4%B8%8E%E5%AE%8C%E7%BE%8E%E5%8C%B9%E9%85%8D/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://avatars1.githubusercontent.com/u/26319720?s=460&amp;v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">39</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">24</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">13</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫性"><span class="toc-number">1.</span> <span class="toc-text">马尔可夫性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫过程"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫奖励过程"><span class="toc-number">3.</span> <span class="toc-text">马尔可夫奖励过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫决策过程"><span class="toc-number">4.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#过程"><span class="toc-number">4.1.</span> <span class="toc-text">过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#状态state"><span class="toc-number">4.2.</span> <span class="toc-text">状态state</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#行为action"><span class="toc-number">4.3.</span> <span class="toc-text">行为action</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#转移概率transition"><span class="toc-number">4.4.</span> <span class="toc-text">转移概率transition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#奖励reward"><span class="toc-number">4.5.</span> <span class="toc-text">奖励reward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#策略Policy"><span class="toc-number">4.6.</span> <span class="toc-text">策略Policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回报Return"><span class="toc-number">4.7.</span> <span class="toc-text">回报Return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#价值函数Value-Function"><span class="toc-number">4.8.</span> <span class="toc-text">价值函数Value Function</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#值函数"><span class="toc-number">5.</span> <span class="toc-text">值函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#状态值函数State-Value-Function"><span class="toc-number">5.1.</span> <span class="toc-text">状态值函数State Value Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#状态——动作值函数"><span class="toc-number">5.2.</span> <span class="toc-text">状态——动作值函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#动态规划"><span class="toc-number">6.</span> <span class="toc-text">动态规划</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#策略迭代"><span class="toc-number">6.1.</span> <span class="toc-text">策略迭代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#值迭代"><span class="toc-number">6.2.</span> <span class="toc-text">值迭代</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#蒙特卡罗"><span class="toc-number">7.</span> <span class="toc-text">蒙特卡罗</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#蒙特卡罗方法"><span class="toc-number">7.1.</span> <span class="toc-text">蒙特卡罗方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#epsilon-贪心法"><span class="toc-number">7.2.</span> <span class="toc-text">$\epsilon$-贪心法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#时序差分学习方法"><span class="toc-number">8.</span> <span class="toc-text">时序差分学习方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SARSA算法"><span class="toc-number">8.1.</span> <span class="toc-text">SARSA算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q学习算法"><span class="toc-number">8.2.</span> <span class="toc-text">Q学习算法</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/../image/markov-decision-process.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Hurley</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">马尔可夫决策过程</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-12-20 00:00:00"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2019-12-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-01-07 20:51:15"><i class="fas fa-history fa-fw"></i> 更新于 2021-01-07</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 6 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h2><p>某一状态信息包含了相关的历史，只要当前状态可知，所有的历史信息都不再需要，当前状态就可以决定未来，则认为该状态具有马尔可夫性（Markov Property）。</p>
<p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/fk6g5r3bL2QnpHZ.png" alt="马尔可夫性"></p>
<a id="more"></a>
<h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><p>又叫马尔可夫链（Markov Chain）。它是一个无记忆的随机过程，可以用一个元组<S, P>表示，其中S是有限数量的状态集，P是状态转移概率矩阵。</p>
<p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/kjVv3wCQdELxm8p.png" alt="马尔可夫过程"></p>
<h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p>马尔可夫奖励过程（Markov Reward Process）在马尔可夫过程的基础上增加了奖励R和衰减系数V：<S, P, R, V>。R是一个奖励函数。S状态下的奖励是某一时刻（t）处所在状态s下在下一个时刻（t+1）能获得的奖励期望：</p>
<script type="math/tex; mode=display">
R_s = E[R_{t+1}|S_t=s]</script><p>衰减系数（Discount Factor）：$\gamma\in[0, 1]$，避免无限循环。</p>
<h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>Markov Decision Process，MDP</p>
<p>多了一个行为集合A，元组<S, A, P, R, V>。</p>
<script type="math/tex; mode=display">
P^a_{ss'} = P[S_{t+1}=s'|S_t=s, A_t=a]\\
R^a_s=E[R_{t+1}|S_t=s, A=a]</script><p>当给定一个MDP： $<S, R, P,R, \gamma>$和一个策略$\pi$，那么状态序列$S_1，S_2$，是一个马尔可夫过程$<S, P^\pi>$。</p>
<p>下一个时刻的状态$S_{t+1}$和<strong>当前时刻的状态$S_t$以及动作$a_t$有关</strong>。</p>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><script type="math/tex; mode=display">
初始化状态agent所处状态s_0\\
\Downarrow\\
根据policy\quad\pi(a|s)采取动作a_0，a_0\sim\pi(a|s_0)\\
\Downarrow\\
根据转移概率p(s'|s,a)采取新状态s_1，s_1\sim p(s'|s,a)\\
\Downarrow\\
得到单步奖励r_1=R^{a_0}_{s_0s_1}\\
\Downarrow\\
持续，得到终止状态S_T，得到轨迹\gamma=(s_0,a_0,s_1,a_1,\dots,s_T)\\
\Downarrow\\
轨迹的联合概率：\\
p(r)=p(S_0)·\prod^\pi_{t=1}p(a_{t-1}|S_{t-1})·p(S_t|S_{t-1},a_{t-1})\\
\Downarrow
对于每一条轨迹，累计奖励函数是关于单步奖励的函数\\
R=f(r_0,r_1\dots r_{T-1})\\
\downarrow\\
可以是T步累计奖励函数R=\sum^{T-1}_{t=0}r_t，\\
也可以是\gamma折扣奖励函数，R=\sum^{T-1}_{t=0}\gamma^t·r_t\\
\Downarrow
期望累计奖励是E_R=E_p(r)[\sum^{T-1}_{t=0}\gamma^t·r_t^T]\\
\therefore agent的目标策略就是使得期望累计奖励最大的策略\\
\pi=\max\limits_{\pi}E_{p(r)}^\pi[\sum^{T-1}_{t=0}\gamma^t·r_t]</script><h3 id="状态state"><a href="#状态state" class="headerlink" title="状态state"></a>状态state</h3><p>agent在每个步骤中所处于的状态集合。</p>
<h3 id="行为action"><a href="#行为action" class="headerlink" title="行为action"></a>行为action</h3><p>agent在每个步骤中所能执行的动作集合。</p>
<h3 id="转移概率transition"><a href="#转移概率transition" class="headerlink" title="转移概率transition"></a>转移概率transition</h3><p>agent处于状态s下，执行动作a后，会转移到状态s’的概率。</p>
<h3 id="奖励reward"><a href="#奖励reward" class="headerlink" title="奖励reward"></a>奖励reward</h3><p>agent处于状态s下，执行动作a后，转移到状态s’后获得的立即奖励值。</p>
<h3 id="策略Policy"><a href="#策略Policy" class="headerlink" title="策略Policy"></a>策略Policy</h3><p>策略$\pi$是概率的集合或分布，其元素$\pi(a|s)$为对过程中的<strong>某一状态s采取可能的行为a的概率</strong>。</p>
<p>agent处于状态s下，应执行动作a的概率。</p>
<p>一个策略定义了个体在各个状态下的各种可能的行为方式以及其概率的大小。</p>
<h3 id="回报Return"><a href="#回报Return" class="headerlink" title="回报Return"></a>回报Return</h3><p>回报$G_t$为在一个马尔可夫奖励链上<strong>从t时刻开始往后所有的奖励的有衰减的总和</strong>。</p>
<h3 id="价值函数Value-Function"><a href="#价值函数Value-Function" class="headerlink" title="价值函数Value Function"></a>价值函数Value Function</h3><p>价值函数给出了某一状态或某一行为的长期价值。</p>
<p>某一状态的价值函数为从该状态开始的马尔可夫链收获的期望。</p>
<p><strong>Bellman Optimality Equation</strong></p>
<p>针对V<em>，一个状态的最优价值等于从该状态出发采取的所有行为产生的行为价值中<em>*最大的</em></em>那个行为价值：</p>
<script type="math/tex; mode=display">
V_*(s)=\max_aq_*(s,a)</script><h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><h3 id="状态值函数State-Value-Function"><a href="#状态值函数State-Value-Function" class="headerlink" title="状态值函数State Value Function"></a>状态值函数State Value Function</h3><p>$V^\pi(s)$为状态值函数，表示从状态s开始，执行策略$\pi$得到的期望总回报：</p>
<script type="math/tex; mode=display">
V^\pi(s)=E_{r\sim p(r)}[\sum^{T-1}_{t=0}\gamma^t·r_{t+1}|\tau_{s_0}=s]</script><p>其中$\tau_{s_0}$表示轨迹$\gamma$的起始状态。</p>
<script type="math/tex; mode=display">
V^\pi(s)=E_{a\sim\pi}(a|s)E_{s'\sim p(s'|s,a)}[r(s,a,s')+\gamma V^\pi(s')]\\
\downarrow</script><p>Bellman equation，表示当前状态的值函数可以通过下个状态的值函数来计算。</p>
<h3 id="状态——动作值函数"><a href="#状态——动作值函数" class="headerlink" title="状态——动作值函数"></a>状态——动作值函数</h3><p>也叫Q函数，Q-function。指初始状态为s并进行动作a，然后执行策略$\pi$得到的期望总回报，即state-action value function。</p>
<script type="math/tex; mode=display">
Q^\pi(s,a)=E_{s'\sim p(s'|s,a)}[r(s,a,s')+\gamma·V^\pi(s')]</script><p>也可以写成：</p>
<script type="math/tex; mode=display">
Q^\pi(s,a)=E_{s'\sim p(s'|s,a)}[r(s,a,s')+\gamma·E_{a'\sim\pi(a'|s')}[Q^\pi(s',a')]]\\
\uparrow\\
Q函数的Bellman方程</script><hr>
<p><strong>基于值函数的策略学习方法</strong></p>
<p>主要分为<strong>动态规划</strong>和<strong>蒙特卡罗</strong>。</p>
<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划又分为<strong>策略迭代（policy iteration）</strong>算法和<strong>值迭代（value iteration）</strong>算法。</p>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><ol>
<li><p>策略评估 policy evaluation</p>
<p>计算当前策略下，每个状态的值函数。可以通过Bellman方程进行迭代计算$V^\pi(s)$。</p>
</li>
<li><p>策略改进 policy improvement</p>
<p>根据值函数更新策略。</p>
</li>
</ol>
<h3 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h3><p>将策略评估与策略改进合并，来直接计算出最优策略。</p>
<p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/oYOnGDUXLir8awP.png" alt="策略迭代 VS 值迭代"></p>
<h2 id="蒙特卡罗"><a href="#蒙特卡罗" class="headerlink" title="蒙特卡罗"></a>蒙特卡罗</h2><p>Q函数。$Q^\pi(s,a)$为初始状态为s，并执行动作a后所能得到的期望总回报。</p>
<script type="math/tex; mode=display">
Q^\pi(s,a)=E_{r\sim p(r)}[G(\tau_{s_0}=s,a_0=a)]</script><p>$\tau_{s_0}=s，a_0=a$表示轨迹$\tau$的起始状态和动作为s，a。</p>
<h3 id="蒙特卡罗方法"><a href="#蒙特卡罗方法" class="headerlink" title="蒙特卡罗方法"></a>蒙特卡罗方法</h3><p>Q函数通过<strong>采样</strong>进行计算。</p>
<p>对于一个策略$\pi$，agent从状态s，执行动作a开始，然后通过随机游走的方法探索环境，并计算其总回报。</p>
<p>在得到Q函数$Q^\pi(s,a)$之后，进行策略改进，在新策略下采样估计Q函数，不断重复。</p>
<h3 id="epsilon-贪心法"><a href="#epsilon-贪心法" class="headerlink" title="$\epsilon$-贪心法"></a>$\epsilon$-贪心法</h3><script type="math/tex; mode=display">
\pi^\epsilon=\begin{cases}
\pi(s),按概率1-\epsilon\\
随机选择\mathcal{A}中的动作，按概率\epsilon
\end{cases}</script><p>将一个仅利用的策略转为带探索的策略，每次选择动作$\pi(s)$的概率为$1-\epsilon+\frac{1}{|\mathcal{A}|}$，其它动作的概率为$\frac{1}{\mathcal{A}}$。</p>
<p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/ZQzg2apVPY5ArCn.png" alt="同策略与异策略"></p>
<h2 id="时序差分学习方法"><a href="#时序差分学习方法" class="headerlink" title="时序差分学习方法"></a>时序差分学习方法</h2><p>蒙特卡罗采样方法一般需要拿到完整的轨迹，才能对策略进行评估并更新模型，因此效率较低。</p>
<p><strong>时序差分学习（temporal-difference learning）</strong>结合了动态规划和蒙特卡罗方法：模拟一段轨迹，每行动一步（或几步）就利用Bellman方程来评估行动前状态的值。（当每次更新动作数为最大数时，就等价于蒙特卡罗方法）。</p>
<h3 id="SARSA算法"><a href="#SARSA算法" class="headerlink" title="SARSA算法"></a>SARSA算法</h3><p><strong>State Action Reward State Action</strong></p>
<p>只需要知道当前状态s和动作a，奖励r(s,a,s’)，下一步的状态s’和动作a’，其采样和优化的策略都是$\pi^\epsilon$，因此是同策略。</p>
<script type="math/tex; mode=display">
Q^\pi(s,a)\longleftarrow Q^\pi(s,a)+\alpha(r(s,a,s')+rQ^\pi(s',a')-Q^\pi(s,a))</script><p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/kKlwCsz3dY85T9g.png" alt="SARSA算法"></p>
<h3 id="Q学习算法"><a href="#Q学习算法" class="headerlink" title="Q学习算法"></a>Q学习算法</h3><p><strong>Q-learning</strong></p>
<script type="math/tex; mode=display">
Q(s,a)\longleftarrow Q(s,a)+\alpha(r+\gamma\max_{a'}Q(s',a')-Q(s,a))</script><p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/07/vE2fCp69J71XB3T.png" alt="Q-learning算法"></p>
<p>与SARSA不同，Q-learning不通过$\pi^\epsilon$来选下一步的动作a’，而是<strong>直接选最优的Q函数</strong>。更新后的Q函数是关于策略$\pi$的，而不是策略$\pi^\epsilon$的。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Hurley</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hurleyjames.github.io/2019/12/20/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">https://hurleyjames.github.io/2019/12/20/马尔可夫决策过程/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hurleyjames.github.io" target="_blank">Hurley</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/">马尔可夫</a></div><div class="post_share"><div class="social-share" data-image="/../image/js-variable.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/images/wechat.JPG" alt="wechat" onclick="window.open('/images/wechat.JPG')"/><div class="post-qr-code__desc">wechat</div></li><li class="reward-item"><img class="post-qr-code__img" src="/images/alipay.JPG" alt="alipay" onclick="window.open('/images/alipay.JPG')"/><div class="post-qr-code__desc">alipay</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/12/23/Timing%20and%20Synchronisation/"><img class="prev-cover" data-src="/../image/timing-and-sync.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Timing and Synchronisation</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/04/%E4%BA%8C%E5%88%86%E5%9B%BE%E4%B8%8E%E5%AE%8C%E7%BE%8E%E5%8C%B9%E9%85%8D/"><img class="next-cover" data-src="/../image/binary-graph.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">二分图与完美匹配</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(/../image/markov-decision-process.png)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2021 By Hurley</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">随时意气风发，独自声势浩大</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>